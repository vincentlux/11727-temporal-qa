{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-26 20:56:30 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-04-26 20:56:30 INFO: Use device: gpu\n",
      "2020-04-26 20:56:30 INFO: Loading: tokenize\n",
      "2020-04-26 20:56:32 INFO: Loading: pos\n",
      "2020-04-26 20:56:33 INFO: Loading: lemma\n",
      "2020-04-26 20:56:33 INFO: Loading: depparse\n",
      "2020-04-26 20:56:34 INFO: Loading: ner\n",
      "2020-04-26 20:56:35 INFO: Done loading processors!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scripts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c85440ce3972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mlmtzr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_normalize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeNormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scripts'"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import json \n",
    "from collections import Counter\n",
    "#import spacy\n",
    "#nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "skip_words = set(stopwords.words('english'))\n",
    "import stanza\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "from scripts.time_normalize import TimeNormalize\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "do_lemmatize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def normalize(text):\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    data = []\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            sample = line.strip().split('\\t')\n",
    "            assert len(sample) == 5\n",
    "            data.append(sample)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1744894\n",
      "29252\n",
      "1744894\n",
      "127539\n",
      "1744894\n",
      "45662\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_garbage(file, lead='verb'):\n",
    "    garbage = {}\n",
    "    concept_len = Counter()\n",
    "    with open(file, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for i, line in enumerate(reader):\n",
    "            if do_lemmatize:\n",
    "                line[0] = lmtzr.lemmatize(line[0].lower())\n",
    "                line[1] = lmtzr.lemmatize(line[1].lower())\n",
    "                line[2] = lmtzr.lemmatize(line[2].lower())\n",
    "            \n",
    "            subj, verb, obj = line[0],line[1], line[2]\n",
    "            if lead == 'verb':\n",
    "                head1, head2, head3 = verb, subj, obj\n",
    "            elif lead == 'subj': \n",
    "                head1, head2, head3 = subj, obj, verb\n",
    "            else:\n",
    "                head1, head2, head3 = obj, subj, verb\n",
    "            if head1 not in garbage:\n",
    "                garbage[head1] = {head2:{head3:[line[3]]}}\n",
    "            elif head2 not in garbage[head1]:\n",
    "                garbage[head1][head2] = {head3:[line[3]]}\n",
    "            elif head3 not in garbage[head1][head2]:\n",
    "                garbage[head1][head2][head3] = [line[3]]\n",
    "            else:\n",
    "                garbage[head1][head2][head3] += [line[3]]\n",
    "        print (i)\n",
    "    print (len(garbage))\n",
    "    return garbage\n",
    "# garbage_verb = read_garbage('/home/kaixinm/kaixinm/Wikipedia/python-sutime/DURATION.csv')\n",
    "# garbage_subj = read_garbage('/home/kaixinm/kaixinm/Wikipedia/python-sutime/DURATION.csv', lead='subj')\n",
    "# garbage_obj = read_garbage('/home/kaixinm/kaixinm/Wikipedia/python-sutime/DURATION.csv', lead='obj')\n",
    "garbage_verb = read_garbage('./DURATION.csv')\n",
    "garbage_subj = read_garbage('./DURATION.csv', lead='subj')\n",
    "garbage_obj = read_garbage('./DURATION.csv', lead='obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'he': {'mile': ['24-hour'], 'best': ['49.17 seconds']}, 'these': {'': ['2007-2018']}, '': {'': ['two weeks', 'four weeks', 'days', 'days', 'four weeks', '440 minutes', 'weeks', 'hours', 'up to 15 minutes', 'six weeks', 'weeks', '45 hours'], 'hour': ['week', 'hours'], 'tree': ['day']}, 'site': {'visitor': ['a day']}, 'it': {'': ['week']}, 'truck': {'mile': ['a year']}, 'rider': {'': ['hours']}, 'census': {'growth': ['ten-year']}, 'she': {'goal': ['37 seconds']}, 'supervisor': {'minute': ['four hour'], '': ['thirty minutes']}, 'bridge': {'': ['days']}, 'lp': {'': ['week'], 'week': ['the past five years']}, 'lifespan': {'': ['seven weeks']}, 'which': {'': ['ten hours']}, 'water': {'': ['several days']}}\n"
     ]
    }
   ],
   "source": [
    "print (garbage_verb['log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_role(label):\n",
    "    print (label)\n",
    "    if 'subj' in label or 'obj' in label:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def extract_info(content):\n",
    "    doc = nlp(content)\n",
    "    query = [[], [], []]\n",
    "    all_words = doc.sentences[0].words\n",
    "    root = [w for w in doc.sentences[0].words if w.deprel == 'root'][0]\n",
    "    if content.startswith('it'):\n",
    "        print (content)\n",
    "        #print (all_words)\n",
    "    if not root.xpos.startswith('VB'):\n",
    "        #print (all_words)\n",
    "        if root.xpos.startswith('JJ'):\n",
    "            #query[1].append(root.text)\n",
    "            for w in doc.sentences[0].words:\n",
    "                if 'root' in w.deprel:\n",
    "                    query[1].append(w.text)\n",
    "                if 'subj' in w.deprel:\n",
    "                    query[0].append(w.text)\n",
    "                if 'nmod' in w.deprel:\n",
    "                    query[2].append(w.text)\n",
    "            #print (content)\n",
    "        else:\n",
    "        #elif root.xpos.startswith('NN'):\n",
    "            #query[0].append(root.text)\n",
    "            for w in doc.sentences[0].words:\n",
    "                if 'root' in w.deprel:\n",
    "                    query[0].append(w.text)\n",
    "                if 'amod' in w.deprel and len(query[0]) != 0:\n",
    "                    query[1].append(w.text)\n",
    "                if 'nmod' in w.deprel and len(query[0]) != 0:\n",
    "                    query[2].append(w.text)\n",
    "            #print (content)\n",
    "        #else:\n",
    "        #    print (all_words)\n",
    "        #if 'last' in content:\n",
    "        #    query[1].append('last')\n",
    "    else:\n",
    "        for w in  doc.sentences[0].words:\n",
    "\n",
    "            if w.head == 0:\n",
    "                query[1].append(w.text)\n",
    "            if (all_words[w.head-1].head == 0 and 'subj' in w.deprel):\n",
    "                query[0].append(w.text)\n",
    "            if (all_words[w.head-1].head == 0 and 'obj' in w.deprel):\n",
    "                query[2].append(w.text)\n",
    "    query = [' '.join(part) for part in query]\n",
    "    #print (query)\n",
    "    return query\n",
    "\n",
    "from collections import defaultdict\n",
    "def find_patterns(data):\n",
    "    # return: dict \n",
    "    # key: question str \n",
    "    # value: tokenized_query: list, level: int, raw_times: list\n",
    "    start = Counter()\n",
    "    prev = ''\n",
    "    question_to_range_map = defaultdict(lambda: defaultdict(dict))\n",
    "    # question_to_range_map = defaultdict()\n",
    "    for sample in data:\n",
    "        if sample[-1] == 'Event Duration':\n",
    "            if prev == sample[1]:\n",
    "                continue\n",
    "            prev = sample[1]\n",
    "            question = nltk.word_tokenize(sample[1].lower())\n",
    "            q_tags = nltk.pos_tag(question)\n",
    "            if do_lemmatize:\n",
    "                question = [lmtzr.lemmatize(word) for word in question]\n",
    "\n",
    "            if 'how' not in question:\n",
    "                content = sample[1].lower()\n",
    "            # hardcode unhandled case for now\n",
    "            elif sample[1] == 'How long will the Earth act like a magnet?':\n",
    "                content = 'earth act like a magnet ?'\n",
    "            elif not q_tags[2][1].startswith('VB') and 'would' not in question:\n",
    "                for i in range(3, len(q_tags)):\n",
    "                    if q_tags[i][1].startswith('VB'):\n",
    "                        break\n",
    "                if len(question)>i+3 and question[i+1] == 'it' and (question[i+2] == 'take' or question[i+2] == 'taken'):\n",
    "                    q_words = ' '.join(question[:i+3])\n",
    "                    content = ' '.join(question[i+3:])\n",
    "                else:\n",
    "                    q_words = ' '.join(question[:i+1])\n",
    "                    content = ' '.join(question[i+1:])\n",
    "            else:\n",
    "                if len(question)>5 and question[3] == 'it' and (question[4] == 'take' or question[4] == 'taken'):\n",
    "                    q_words = ' '.join(question[:5])\n",
    "                    content = ' '.join(question[5:])\n",
    "                else:\n",
    "                    q_words = ' '.join(question[:3])\n",
    "                    content = ' '.join(question[3:])\n",
    "            start[q_words] += 1\n",
    "            if len(content) == 0:\n",
    "                continue\n",
    "            # ['minority', 'remained', '']\n",
    "            query = extract_info(content)\n",
    "            level, results = search_in_garbage(query, garbage_verb)\n",
    "            if level == 2:\n",
    "                subj_level, subj_results = search_in_garbage_subj(query, garbage_subj, garbage_obj)\n",
    "                if subj_level == 2:\n",
    "                    results += subj_results\n",
    "            elif level == 1:\n",
    "                subj_level, subj_results = search_in_garbage_subj(query, garbage_subj, garbage_obj)\n",
    "                if subj_level == 2:\n",
    "                    results = subj_results \n",
    "                    level = subj_level\n",
    "                elif subj_level == 1:\n",
    "                    results += subj_results\n",
    "            elif level == 0:\n",
    "                level, results = search_in_garbage_subj(query, garbage_subj, garbage_obj)\n",
    "#             print (query)\n",
    "#             print (level)\n",
    "#             print (results)\n",
    "#             break\n",
    "            \n",
    "            question_to_range_map[sample[1]]['tokenized_query'] = query\n",
    "            question_to_range_map[sample[1]]['level'] = level\n",
    "            # question_to_range_map[sample[1]]['raw_times'] = results\n",
    "            question_to_range_map[sample[1]]['raw_times'] = [normalize(i.lower().replace('-', ' ')) for i in results]\n",
    "            # start converting results to range\n",
    "    #print (start)\n",
    "    return question_to_range_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_garbage(query, garbage):\n",
    "#     import pdb; pdb.set_trace()\n",
    "    if query[1] != '' and query[1] in garbage:\n",
    "        if query[0] != '' and query[0] in garbage[query[1]]:\n",
    "            if query[2] != '' and query[2] in garbage[query[1]][query[0]]:\n",
    "                results = garbage[query[1]][query[0]][query[2]]      # all 3 words matched\n",
    "                return 3, results   # how many words matched\n",
    "            elif query[2] != '':   # there exists object but not matched \n",
    "                results = [vv for obj, v in garbage[query[1]][query[0]].items() for vv in v]  # stuff matched by verb and subj\n",
    "                for subj, d in garbage[query[1]].items():  # search for all that could be matched by verb and obj\n",
    "                    for obj, v in d.items():\n",
    "                        if obj == query[2]:\n",
    "                            results += v\n",
    "                return 2, results     # all level 2 matched\n",
    "            else:\n",
    "                results = [vv for obj, v in garbage[query[1]][query[0]].items() for vv in v]  # only has verb and subj, already the best we can do\n",
    "                return 2, results \n",
    "        elif query[2] != '':  # subj not matched and obj exist\n",
    "            results = []\n",
    "            for subj, d in garbage[query[1]].items():  # search for all that could be matched by verb and obj\n",
    "                for obj, v in d.items():\n",
    "                    if obj == query[2]:\n",
    "                        results += v\n",
    "            if len(results) > 0:\n",
    "                return 2, results\n",
    "            else:        # the best we can do is matching only with verb\n",
    "                for subj, d in garbage[query[1]].items():\n",
    "                    for obj, v in d.items():\n",
    "                        results += v\n",
    "                return 1, results\n",
    "        else:   # the best we can do is matching only with verb\n",
    "            results = []\n",
    "            for subj, d in garbage[query[1]].items():\n",
    "                for obj, v in d.items():\n",
    "                    results += v\n",
    "            return 1, results\n",
    "    return 0, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_garbage_subj(query, garbage, next_garbage):\n",
    "    if query[0] != '' and query[0] in garbage:\n",
    "        if query[2] != '' and query[2] in garbage[query[0]]:  # we search for subj and obj match\n",
    "                results = [vv for obj, v in garbage[query[0]][query[2]].items() for vv in v]\n",
    "                return 2, results \n",
    "        elif query[2] != '':  # obj not matched but exist\n",
    "            results = []   # all matched with subj\n",
    "            for obj, d in garbage[query[0]].items():\n",
    "                for verb, v in d.items():\n",
    "                    results += v\n",
    "            results += search_in_garbage_obj(query, next_garbage)\n",
    "            return 1, results\n",
    "        else:   # the best we can do is matching only with subj\n",
    "            results = []\n",
    "            for obj, d in garbage[query[0]].items():\n",
    "                for verb, v in d.items():\n",
    "                    results += v\n",
    "            return 1, results\n",
    "    elif query[2] != '':\n",
    "        results = []\n",
    "        results += search_in_garbage_obj(query, next_garbage)\n",
    "        return 1, results\n",
    "    return 0, []\n",
    "def search_in_garbage_obj(query, garbage):\n",
    "    if query[2] != '' and query[2] in garbage:\n",
    "        results = []\n",
    "        for subj, d in garbage[query[2]].items():\n",
    "            for verb, v in d.items():\n",
    "                results += v\n",
    "        return results\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_map_with_range(question_to_range_map):\n",
    "    unit_keys = ['seconds', 'minutes', 'hours', 'days', 'weeks', 'months', 'years', 'decades', 'centuries']\n",
    "    if do_lemmatize:\n",
    "        unit_keys = [lmtzr.lemmatize(key) for key in unit_keys]\n",
    "    for k, v in question_to_range_map.items():\n",
    "        unit_count_map = defaultdict(lambda:0, {k: 0 for k in unit_keys})\n",
    "        # raw_times = [normalize(i.replace('-', ' ')) for i in v['raw_times']]\n",
    "        raw_times = v['raw_times']\n",
    "        for time in raw_times:\n",
    "            time_tokenized = nltk.word_tokenize(time)\n",
    "            if do_lemmatize:\n",
    "                time_tokenized = [lmtzr.lemmatize(tok) for tok in time_tokenized]\n",
    "            for tok in time_tokenized:\n",
    "                if tok in unit_count_map:\n",
    "                    unit_count_map[tok] += 1\n",
    "\n",
    "        # sort unit_count_map desc\n",
    "        if second_sort_by_desc_unit:\n",
    "            unit_count_sorted = sorted(unit_count_map.items(), key=lambda v: (v[1], unit_keys.index(v[0])), reverse=True)\n",
    "        else:\n",
    "            unit_count_sorted = sorted(unit_count_map.items(), key=lambda v: v[1], reverse=True)\n",
    "        question_to_range_map[k]['unit_count_sorted'] = unit_count_sorted\n",
    "        question_to_range_map[k]['max_unit'] = unit_count_sorted[0]\n",
    "        question_to_range_map[k]['min_unit'] = unit_count_sorted[1]\n",
    "        # generate range\n",
    "        if unit_keys.index(unit_count_sorted[0][0]) < \\\n",
    "            unit_keys.index(unit_count_sorted[1][0]):\n",
    "            all_ranged_units = unit_keys[unit_keys.index(unit_count_sorted[0][0]):unit_keys.index(unit_count_sorted[1][0])+1]\n",
    "        else:\n",
    "            all_ranged_units = unit_keys[unit_keys.index(unit_count_sorted[1][0]):unit_keys.index(unit_count_sorted[0][0])+1]\n",
    "            \n",
    "        question_to_range_map[k]['range'] = all_ranged_units\n",
    "    return question_to_range_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_from_range(norm_data, range_map):\n",
    "    results = []\n",
    "    for sample in norm_data:\n",
    "        if sample[-1] == 'Event Duration':\n",
    "            ranged_units = range_map[sample[1]]['range']\n",
    "            answer = nltk.word_tokenize(sample[2].lower())\n",
    "            answer = [lmtzr.lemmatize(word) for word in answer]\n",
    "            found = False\n",
    "            for tok in answer:\n",
    "                if tok in ranged_units:\n",
    "                    found = True\n",
    "                    break\n",
    "            results.append('yes' if found else 'no')\n",
    "    return results\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_lemmatize = True\n",
    "second_sort_by_desc_unit = False\n",
    "# data = read_data('../../../MCTACO/dataset/test_9442.tsv')\n",
    "data = read_data('./MCTACO/dataset/dev_3783.tsv')\n",
    "question_to_time_map = find_patterns(data)\n",
    "question_to_range_map = expand_map_with_range(question_to_time_map)\n",
    "normalized_data = read_data('./MCTACO/dataset/normalized_dev_3783.tsv')\n",
    "res = get_result_from_range(normalized_data, question_to_range_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./MCTACO/dataset/garbage_duration_result.txt', 'w') as f:\n",
    "    for ans in res:\n",
    "        f.write(ans+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(15213)\n",
    "def dump_baselines(res):\n",
    "    with open('../../../MCTACO/dataset/random_result.txt', 'w') as f:\n",
    "        for ans in res:\n",
    "            if random.random() > 0.5:\n",
    "                f.write('yes\\n')\n",
    "            else:\n",
    "                f.write('no\\n')\n",
    "    with open('../../../MCTACO/dataset/positive_result.txt', 'w') as f:\n",
    "        for ans in res:\n",
    "            f.write('yes\\n')\n",
    "    with open('../../../MCTACO/dataset/negative_result.txt', 'w') as f:\n",
    "        for ans in res:\n",
    "            f.write('no\\n')\n",
    "dump_baselines(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dev random\n",
    "# Strict Acc.: 0.031746031746031744\n",
    "# Avg F1: 0.3056352377780948\n",
    "# Dev negative\n",
    "# Strict Acc.: 0.24603174603174602\n",
    "# Avg F1: 0.24603174603174602\n",
    "# Dev positive\n",
    "# Strict Acc.: 0.015873015873015872\n",
    "# Avg F1: 0.3305273982573848\n",
    "# Dev garbage \n",
    "# Strict Acc.: 0.19047619047619047\n",
    "# Avg F1: 0.38031987389756794\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test positive \n",
    "# Strict Acc.: 0.022292993630573247\n",
    "# Avg F1: 0.37325883850538083\n",
    "# Test negative\n",
    "# Strict Acc.: 0.2197452229299363\n",
    "# Avg F1: 0.2197452229299363\n",
    "# Test random \n",
    "# Strict Acc.: 0.028662420382165606\n",
    "# Avg F1: 0.28177336796669894\n",
    "# Test garbage \n",
    "# Strict Acc.: 0.15605095541401273\n",
    "# Avg F1: 0.3671067949883198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. lemmatize garbage subj/obj/verb\n",
    "# 2. start generating ranges for each Q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(filename, data):\n",
    "    with open(filename, 'w') as fout:\n",
    "        json.dump(data, fout)\n",
    "write_data('dev_3783_garbage_part1.json', question_to_time_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize all extracted patterns to seconds and\n",
    " 1. use min max as pure range for rule based extraction\n",
    " 2. use (min, max, avg, std) as feature for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = TimeNormalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert question_to_range_map \n",
    "import statistics\n",
    "\n",
    "def expand_map_with_seconds_stats(norm, question_to_range_map, add_quantization=False, drop_century=True):\n",
    "    \"\"\"expand dictionary and add fields 'converted_seconds', 'min', 'max', 'avg', 'std', 'quantization'(optional)\n",
    "    norm(object)\n",
    "    question_to_range_map(dict) \n",
    "    \"\"\"\n",
    "    if add_quantization:\n",
    "        raise NotImplementedError\n",
    "    for k, v in question_to_range_map.items():\n",
    "        question_to_range_map[k]['converted_seconds'] = []\n",
    "        for t in v['raw_times']:\n",
    "            if drop_century and ('century' in t or 'centuries' in t):\n",
    "                continue\n",
    "            success, second = norm.convert_time_to_seconds(t)\n",
    "            if success:\n",
    "                question_to_range_map[k]['converted_seconds'].append(second)\n",
    "        \n",
    "        # get stats\n",
    "        try:\n",
    "            question_to_range_map[k]['min'] = min(question_to_range_map[k]['converted_seconds'])\n",
    "            question_to_range_map[k]['max'] = max(question_to_range_map[k]['converted_seconds'])\n",
    "            question_to_range_map[k]['avg'] = statistics.mean(question_to_range_map[k]['converted_seconds'])\n",
    "            question_to_range_map[k]['std'] = statistics.stdev(question_to_range_map[k]['converted_seconds'])\n",
    "        except ValueError:\n",
    "            if len(question_to_range_map[k]['converted_seconds']) == 1:\n",
    "                question_to_range_map[k]['std'] = 0.0\n",
    "            elif len(question_to_range_map[k]['converted_seconds']) == 0:\n",
    "                question_to_range_map[k]['min'] = 0.0\n",
    "                question_to_range_map[k]['max'] = 0.0\n",
    "                question_to_range_map[k]['avg'] = 0.0\n",
    "                question_to_range_map[k]['std'] = 0.0\n",
    "            else:\n",
    "                print(k)\n",
    "                print(question_to_range_map[k])\n",
    "#         if add_quantization:\n",
    "            # quantize by ['seconds', 'minutes', 'hours', 'days', 'weeks', 'months', 'years', 'decades', 'centuries']\n",
    "            \n",
    "            \n",
    "    return question_to_range_map\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_from_seconds_min_max(data, range_map):\n",
    "    results = []\n",
    "    for sample in data:\n",
    "        if sample[-1] == 'Event Duration':\n",
    "            min_second = range_map[sample[1]]['min']\n",
    "            max_second = range_map[sample[1]]['max']\n",
    "            answer = float(sample[2])\n",
    "            if min_second <= answer <= max_second:\n",
    "                results.append('yes')        \n",
    "            else:\n",
    "                results.append('no')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_eval_file(result, path):\n",
    "    with open(path, 'w') as f:\n",
    "        for ans in result:\n",
    "            f.write(ans+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. use min max as pure range for rule based extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_to_range_map = expand_map_with_seconds_stats(norm, question_to_range_map, add_quantization=False, drop_century=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data('./MCTACO/dataset/seconds_dev_event_duration.tsv')\n",
    "res = get_result_from_seconds_min_max(data, question_to_range_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './MCTACO/dataset/min_max_seconds.txt'\n",
    "write_eval_file(res, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Acc.: 0.09523809523809523\n",
      "Avg F1: 0.3410240889250913\n"
     ]
    }
   ],
   "source": [
    "!python evaluator.py eval --test_file MCTACO/dataset/normalized_dev_event_duration.tsv --prediction_file MCTACO/dataset/min_max_seconds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. use (min, max, avg, std) as feature for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert map to pandas\n",
    "def convert_map_to_matrix(range_map, data, unit_fields, additional_feature_fields=['min', 'max', 'avg', 'std']):\n",
    "    rows = []\n",
    "    \n",
    "    for sample in data:\n",
    "        row = []\n",
    "        if sample[-1] == 'Event Duration':\n",
    "            exist_fields = [i[0] for i in range_map[sample[1]]['unit_count_sorted']]\n",
    "            for f in unit_fields:\n",
    "                if f in exist_fields:\n",
    "                    # find its index from sorted list and get counts\n",
    "                    index = exist_fields.index(f)\n",
    "                    row.append(range_map[sample[1]]['unit_count_sorted'][index][1])\n",
    "                else:\n",
    "                    row.append(0)\n",
    "    \n",
    "            \n",
    "            for field in additional_feature_fields:\n",
    "                row.append(range_map[sample[1]][field])\n",
    "            # append seconds of this instance\n",
    "            row.append(float(sample[2]))\n",
    "            # append label to the end\n",
    "            if sample[-2] == 'yes':\n",
    "                row.append(1)\n",
    "            else:\n",
    "                row.append(0)\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it existed ?\n",
      "it last ?\n",
      "it a violent world\n"
     ]
    }
   ],
   "source": [
    "do_lemmatize = True\n",
    "second_sort_by_desc_unit = False\n",
    "unit_fields = ['second', 'minute', 'hour', 'day', 'week', 'month', 'year', 'decade', 'century']\n",
    "stats_fields = ['min', 'max', 'avg', 'std']\n",
    "train_data = read_data('./MCTACO/dataset/seconds_dev_event_duration.tsv')\n",
    "train_question_to_time_map = find_patterns(train_data)\n",
    "train_question_to_range_map = expand_map_with_range(train_question_to_time_map)\n",
    "train_question_to_range_map = expand_map_with_seconds_stats(norm, train_question_to_range_map, add_quantization=False, drop_century=True)\n",
    "train_rows = convert_map_to_matrix(train_question_to_range_map, train_data, unit_fields, additional_feature_fields=stats_fields)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_data = read_data('./MCTACO/dataset/seconds_test_event_duration.tsv')\n",
    "test_question_to_time_map = find_patterns(test_data)\n",
    "test_question_to_range_map = expand_map_with_range(test_question_to_time_map)\n",
    "test_question_to_range_map = expand_map_with_seconds_stats(norm, test_question_to_range_map, add_quantization=False, drop_century=True)\n",
    "test_rows = convert_map_to_matrix(test_question_to_range_map, test_data, unit_fields, additional_feature_fields=stats_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fields = unit_fields + stats_fields + ['candidate_seconds','label']\n",
    "train_df = pd.DataFrame(train_rows,columns=all_fields)\n",
    "test_df = pd.DataFrame(test_rows,columns=all_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>second</th>\n",
       "      <th>minute</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>decade</th>\n",
       "      <th>century</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>avg</th>\n",
       "      <th>std</th>\n",
       "      <th>candidate_seconds</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.892160e+10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.838240e+09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.555200e+07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.600000e+02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.576800e+09</td>\n",
       "      <td>1.874131e+08</td>\n",
       "      <td>3.370173e+08</td>\n",
       "      <td>2.592000e+07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.576800e+09</td>\n",
       "      <td>1.874131e+08</td>\n",
       "      <td>3.370173e+08</td>\n",
       "      <td>3.000000e+02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.576800e+09</td>\n",
       "      <td>1.874131e+08</td>\n",
       "      <td>3.370173e+08</td>\n",
       "      <td>6.000000e+02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.576800e+09</td>\n",
       "      <td>1.874131e+08</td>\n",
       "      <td>3.370173e+08</td>\n",
       "      <td>8.640000e+05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.576800e+09</td>\n",
       "      <td>1.874131e+08</td>\n",
       "      <td>3.370173e+08</td>\n",
       "      <td>1.800000e+03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     second  minute  hour  day  week  month  year  decade  century  min  \\\n",
       "0         0       0     0    0     0      0     1       0        0  0.0   \n",
       "1         0       0     0    0     0      0     1       0        0  0.0   \n",
       "2         0       0     0    0     0      0     1       0        0  0.0   \n",
       "3         0       0     0    0     0      0     1       0        0  0.0   \n",
       "4         0       0     0    0     0      0     1       0        0  0.0   \n",
       "..      ...     ...   ...  ...   ...    ...   ...     ...      ...  ...   \n",
       "195       1       1     7   30    10     10    78       3        0  3.0   \n",
       "196       1       1     7   30    10     10    78       3        0  3.0   \n",
       "197       1       1     7   30    10     10    78       3        0  3.0   \n",
       "198       1       1     7   30    10     10    78       3        0  3.0   \n",
       "199       1       1     7   30    10     10    78       3        0  3.0   \n",
       "\n",
       "              max           avg           std  candidate_seconds  label  \n",
       "0    0.000000e+00  0.000000e+00  0.000000e+00       3.000000e+01      0  \n",
       "1    0.000000e+00  0.000000e+00  0.000000e+00       1.892160e+10      0  \n",
       "2    0.000000e+00  0.000000e+00  0.000000e+00       2.838240e+09      0  \n",
       "3    0.000000e+00  0.000000e+00  0.000000e+00       1.555200e+07      1  \n",
       "4    0.000000e+00  0.000000e+00  0.000000e+00       3.600000e+02      0  \n",
       "..            ...           ...           ...                ...    ...  \n",
       "195  1.576800e+09  1.874131e+08  3.370173e+08       2.592000e+07      0  \n",
       "196  1.576800e+09  1.874131e+08  3.370173e+08       3.000000e+02      1  \n",
       "197  1.576800e+09  1.874131e+08  3.370173e+08       6.000000e+02      1  \n",
       "198  1.576800e+09  1.874131e+08  3.370173e+08       8.640000e+05      0  \n",
       "199  1.576800e+09  1.874131e+08  3.370173e+08       1.800000e+03      1  \n",
       "\n",
       "[200 rows x 15 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df.drop('label',axis = 1)\n",
    "train_y = train_df.label\n",
    "test_x = test_df.drop('label', axis=1)\n",
    "test_y = test_df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaopen2/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression()\n",
    "# clf = svm.SVC()\n",
    "# clf.fit(train_x, train_y)\n",
    "# pred = clf.predict(test_x)\n",
    "\n",
    "logistic_regression.fit(train_x, train_y)\n",
    "pred_y = logistic_regression.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7404353562005277"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(test_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in pred_y if i == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How long will the Earth act like a magnet?\n",
      "defaultdict(<class 'dict'>, {'unit_count_sorted': {}, 'min': {}, 'max': {}, 'avg': {}, 'std': {}})\n"
     ]
    }
   ],
   "source": [
    "# for k, v in test_question_to_range_map.items():\n",
    "# #     if len(v['min'])==0 or len(v['max'])==0 or len(v['avg'])==0 or len(v['std'])==0:\n",
    "#     try:\n",
    "#         a = float(v['min'])\n",
    "#         a = float(v['max'])\n",
    "#         a = float(v['avg'])\n",
    "#         a = float(v['std'])\n",
    "#     except:\n",
    "#         print(k)\n",
    "#         print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
